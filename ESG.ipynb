{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aim: Obtain ESG Ratings for Biggest Companies in the World automatically\n",
    "\n",
    "# Reason: ESG Data is a form of alternative data that can be used to analyse companies\n",
    "# In a world constantly facing enviromental challenges and social issues; companies that have promising ESG data\n",
    "# will be rewarding to support; and indeed invest in- thus the produced datasheet aims to reflect how diff companies\n",
    "# around the world score on their ESG; and provide another layer of analysis on which companies are worth investing in.\n",
    "\n",
    "###\n",
    "import pandas as pd\n",
    "import requests  \n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium import webdriver\n",
    "import time\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=1 # can be modified to no greater than 20\n",
    "if index>20:\n",
    "    print('Highest Page number is 20')\n",
    "    index=20\n",
    "end=1 # can be modified for more pages (but it takes approx 10 minutes per page; advise keeping index and end numbers within difference of 2)\n",
    "if end>20:\n",
    "    end=20\n",
    "if end<index: # prevents end from being smaller than index\n",
    "    print('End is less than starting index.\\nEnd has been set to the same as index.')\n",
    "    end=index\n",
    "\n",
    "start='https://disfold.com/world/companies/?page='\n",
    "\n",
    "# WHY Disfold?\n",
    "# > webpage that provides  extensive data on biggest companies (by market cap) around the world\n",
    "# > readily supplied with good information; including Market Cap, Stock of Company and Country in which it originates and/or operates primarily\n",
    "# > data is also updated regularly\n",
    "\n",
    "\n",
    "og=pd.DataFrame()\n",
    "row_n=1\n",
    "while index<=end: # paginates until no more pages can be found\n",
    "    url=start+str(index) # goes to specific page of url in start\n",
    "    r=(requests.get(url)) \n",
    "\n",
    "    if r.status_code != 200: # if page does not give an ok response, exits code entirely\n",
    "        out=False\n",
    "        print(f'Code exited. HTTP status code {r.status_code} detected.')\n",
    "        break\n",
    "    else:\n",
    "        out=True\n",
    "        dic={}\n",
    "        soup=BeautifulSoup(r.text,'html.parser') # extracts HTML source code\n",
    "        \n",
    "        for e in (soup.find_all('tr')[1:]): # used to obtain all rows in the table of the website\n",
    "            lis=(e.text.split('\\n')) # for each row; text information is extracted and separated into a list\n",
    "            n_lis=[]\n",
    "            for i in lis:\n",
    "                if i.strip()=='':\n",
    "                    pass \n",
    "                else:\n",
    "                    i=i.strip() # remves whitespace from elements\n",
    "                    n_lis.append(i)\n",
    "            company=n_lis[1] # company name\n",
    "            market_cap=n_lis[2] # market cap of company\n",
    "            stock=n_lis[3]  # stock of company\n",
    "            country=n_lis[4] # country in which company operates primarily\n",
    "            sector=n_lis[5] # Sector in which company primarily works in\n",
    "            industry=n_lis[6] # specific industry in which company deals in\n",
    "\n",
    "            if country=='United States': # used to query company info on another site (Morningstar> which has ESG info)\n",
    "                # based on location of company and it's stock name\n",
    "                stock=stock.replace('.','')\n",
    "                query='https://www.morningstar.com/search/us-securities?query='+stock\n",
    "            else:\n",
    "                stock=stock.replace('.','')\n",
    "                query='https://www.morningstar.com/search/foreign-securities?query='+stock\n",
    "            \n",
    "            # WHY Morningstar?\n",
    "\n",
    "            # > financial firm service that provides investment research and management services\n",
    "            # > having a 40% ownership stake in sustainalytics; has access to ESG data of more than 16,000 companies across the market\n",
    "            # > based on a reliable industry standard\n",
    "            # > in terms of webscraping; relatively easy to parse through multiple companies\n",
    "            # > is also free to use (no need for pay for view)\n",
    "\n",
    "            search_q=requests.get(query)\n",
    "            q_soup=BeautifulSoup(search_q.text,'html.parser') # extracts HTML source code\n",
    "            all_href=(q_soup.findAll('a',href=True))\n",
    "            for e in all_href:\n",
    "                if '/stocks/' in str(e):\n",
    "                    stock_url=(e['href'])[:-5] # finds the necessary link for the particular stock\n",
    "                    break\n",
    "            \n",
    "            esg_query='https://www.morningstar.com'+stock_url+'sustainability' # goes to the sustainability aspect of the site for the specific\n",
    "            # stock; thereby showing info on the ESG rating for the company \n",
    "\n",
    "            dr=webdriver.Chrome(ChromeDriverManager().install()) # extracts dynamically loaded info (the main reason for why the code runs long)\n",
    "            dr.get(esg_query)\n",
    "            time.sleep(2) # allows for webpage to load properly (2s is safest bet btw running code quickly and getting optimal results)\n",
    "            try:\n",
    "                number=dr.find_element(by=By.CLASS_NAME, value=\"text-value\") # extracts risk value\n",
    "                esg_number=float(str(number.get_attribute('innerHTML')).strip())\n",
    "                category=dr.find_element(by=By.CLASS_NAME, value=\"text-category\") # extracts risk group\n",
    "                esg_category=str(category.get_attribute('innerHTML')).strip()\n",
    "                \n",
    "                # esg_risk_rating=len(dr.find_elements(by=By.CLASS_NAME, value=\"sal-eqsv-sustainability-rate.rating\")) \n",
    "                # linked to category group> higher number tallied with lower risk\n",
    "                # deemed unnecessary\n",
    "            except:\n",
    "                row=[company,stock,market_cap,country,sector,industry,None,None] # if error is raised while extracting info in the try block; last 2 columns \n",
    "                # given None values\n",
    "            else:\n",
    "                row=[company,stock,market_cap,country,sector,industry,esg_number,esg_category] # if no error\n",
    "                # last 2 columns filled with respective values\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            # The older; faster; code \n",
    "            # Issue was due to large number of requests in short period of time on Google search\n",
    "\n",
    "            query=f'https://www.google.com/search?q=\"sustainalytics\" {company} esg'\n",
    "            dr.get(query) # PROBLEM> too many requests to google will just not validate it\n",
    "\n",
    "            # Required for url to show up as one of the first few search results\n",
    "\n",
    "            # Could not search on site directly owing to need for an id to be added to the url\n",
    "\n",
    "            for e in BeautifulSoup(dr.page_source,'html.parser').findAll('a',href=True):\n",
    "                if 'https://www.sustainalytics.com/esg-rating' in (e['href']):\n",
    "                    search=re.findall('(https://.*?)&',str(e['href']))[0]\n",
    "                    break\n",
    "            else:\n",
    "                search=False\n",
    "            if search:\n",
    "                search_url=search \n",
    "                r=requests.get(search_url)\n",
    "                print(r)\n",
    "                risk_score=float(BeautifulSoup(r.text,'html.parser').find('div',{'class':'col-6 risk-rating-score'}).text)\n",
    "                print(risk_score)\n",
    "                risk_rating=(BeautifulSoup(r.text,'html.parser').find('div',{'class':'col-6 risk-rating-assessment'}).text)\n",
    "                group=(BeautifulSoup(r.text,'html.parser').find('strong',{'class':'industry-group'}).text)\n",
    "                row=[company,stock,market_cap,country,risk_score,risk_rating,group]\n",
    "                #print('y')\n",
    "            else:\n",
    "                row=[company,stock,market_cap,country,None,None,None]\n",
    "                #print('n')\n",
    "                \"\"\"            \n",
    "            dic[row_n]=row \n",
    "            row_n+=1\n",
    "        df = pd.DataFrame.from_dict(dic,orient='index')\n",
    "        \n",
    "    index+=1\n",
    "    og=pd.concat([og,df],axis=0)\n",
    "if out:\n",
    "    og.columns=['Company','Stock','Market_Cap','Country','Sector','Industry','Risk_Value','Risk_Rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og # To display how the dataframe looks like in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og.to_excel('Company_ESG_Info.xlsx') # exports the collated data to an excel sheet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f5816e6188795d1cc8afa4306b69c5e3a103f064a75e820fe776234e4cf83e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
