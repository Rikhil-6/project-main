{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification, Topic Modelling and Text Generation\n",
    "\n",
    "**Hello World!** and welcome to this project created by me, Rikhil Singh. As established by the above title, this project aims to dive into the realm of three main areas of Natural Language processing (NLP) - from the humble classification of texts, to the unsupervised modelling of \n",
    "these texts (henceforth referred to as articles) into particular topics - as well as the generation of specific words in a particular \n",
    "order to give brief yet accurate representations of these articles to the layman. \n",
    "\n",
    "The applicability of these 3 broad spheres of NLP\n",
    "can be extended to multiple realms - including sentiment analysis, document tagging as well as automated title generation - and is \n",
    "composed of Machine Learning (ML) models that I've built from scratch, as well as pretrained ML Algorithms optimised for beneficial\n",
    "performance. It is advisable to run this notebook not in totality - but rather in the 3 main segments which will be outlined by more\n",
    "markdowns that can be found as you parse the code. It is imperative however to run the code cell directly below in order to import the \n",
    "necessary libraries utilised by this code as well as to amend the `file_path` variable in the cell as need be. \n",
    "\n",
    "Should some of the libraries not be installed - please pip install the `requirements.txt` file \n",
    "found in the main directory where this code file, as well as the dataset, can be found. Happy Coding!\n",
    "\n",
    "7-12-2024 - 23-12-2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ensure that this code cell is the first to be run before all others\n",
    "## Each Section (1. 2. & 3.) can be run independently of each other so long as THIS cell has been run first\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as D\n",
    "\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "from datasets import Dataset\n",
    "from bertopic import BERTopic\n",
    "from rouge_score import rouge_scorer\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import precision_score, f1_score, classification_report\n",
    "\n",
    "file_path = \"bbc-news-data.csv\" # amend the file path for the dataset as necessary\n",
    "## Dataset Source - https://www.kaggle.com/datasets/hgultekin/bbcnewsarchive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Text Classification\n",
    "\n",
    "Classification, while normally thought of to be a relatively simple and trivial task, can be quite nuanced and significant when it comes to the world we live in today. With information constantly flowing through multiple channels - the classification of text - whether it be for articles into sports, politics, or entertainment - to filtering fake news from legitimate sources, the applications of text classification extends far and wide, as it finds utility across multiple domains. Here, we aim to compare how well the articles can be classified into the FIVE predetermined categories already created - not solely through the content of an entire article, but even through its plain and simple title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_path,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2225 entries, 0 to 2224\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   category  2225 non-null   object\n",
      " 1   filename  2225 non-null   object\n",
      " 2   title     2225 non-null   object\n",
      " 3   content   2225 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 69.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "sport            22.966292\n",
       "business         22.921348\n",
       "politics         18.741573\n",
       "tech             18.022472\n",
       "entertainment    17.348315\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category'].value_counts(normalize=True)*100 # Fortunately, this dataset is quite well balanced from the get-go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>filename</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>001.txt</td>\n",
       "      <td>Ad sales boost Time Warner profit</td>\n",
       "      <td>Quarterly profits at US media giant TimeWarne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>002.txt</td>\n",
       "      <td>Dollar gains on Greenspan speech</td>\n",
       "      <td>The dollar has hit its highest level against ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>003.txt</td>\n",
       "      <td>Yukos unit buyer faces loan claim</td>\n",
       "      <td>The owners of embattled Russian oil giant Yuk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>004.txt</td>\n",
       "      <td>High fuel prices hit BA's profits</td>\n",
       "      <td>British Airways has blamed high fuel prices f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>005.txt</td>\n",
       "      <td>Pernod takeover talk lifts Domecq</td>\n",
       "      <td>Shares in UK drinks and food firm Allied Dome...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category filename                              title  \\\n",
       "0  business  001.txt  Ad sales boost Time Warner profit   \n",
       "1  business  002.txt   Dollar gains on Greenspan speech   \n",
       "2  business  003.txt  Yukos unit buyer faces loan claim   \n",
       "3  business  004.txt  High fuel prices hit BA's profits   \n",
       "4  business  005.txt  Pernod takeover talk lifts Domecq   \n",
       "\n",
       "                                             content  \n",
       "0   Quarterly profits at US media giant TimeWarne...  \n",
       "1   The dollar has hit its highest level against ...  \n",
       "2   The owners of embattled Russian oil giant Yuk...  \n",
       "3   British Airways has blamed high fuel prices f...  \n",
       "4   Shares in UK drinks and food firm Allied Dome...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_state = 27\n",
    "train_df,test_df = train_test_split(df,train_size=0.8,random_state=r_state,stratify = df['category']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate(df):\n",
    "    return df['title']+'.'+df['content']\n",
    "train_df['title_and_content'] = consolidate(train_df) # this column was mainly created for the cohesiveness of the title and article itself\n",
    "# it is not used in the code below but can be - as will be explained further on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(train_df,train_size=0.75,random_state=r_state,stratify=train_df['category']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'business': 0, 'entertainment': 1, 'politics': 2, 'sport': 3, 'tech': 4}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping = {k:i for i,k in enumerate(sorted(train['category'].unique()))}\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['enc_category'] = train['category'].map(mapping)\n",
    "val['enc_category'] = val['category'].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\") # spacy's (medium) English pipeline is used for the tokenisation and embedding of texts\n",
    "# it is perferred to other pipelines owing to its relative ease of usage alongside its compact size but capable strengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['title_embed'] = train['title'].apply(lambda x: torch.from_numpy(nlp(x.lower()).vector))\n",
    "val['title_embed'] = val['title'].apply(lambda x: torch.from_numpy(nlp(x.lower()).vector))\n",
    "# While word embeddings may seem to be the natural fix - particularly for something like a title, \n",
    "# document embeddings prove to be quite efficient while also being standardised in terms of length as well as \n",
    "# retaining semantic information distinctly well even in short corpuses - though this is improved in longer articles\n",
    "# https://orangedatamining.com/blog/embedding-vs-bow/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['embed_content'] = train['content'].apply(lambda x: torch.from_numpy(nlp(x.lower()).vector))\n",
    "val['embed_content'] = val['content'].apply(lambda x: torch.from_numpy(nlp(x.lower()).vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cat = \"title_embed\" # While the feature of interest henceforth is the embedded title,\n",
    "# this can be amended to \"embed_content\" - which also yields much more impressive results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train[[feature_cat,\"enc_category\"]]\n",
    "val_data = val[[feature_cat,\"enc_category\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameDataset(D): # Creating a suitable dataset for pytorch to read from\n",
    "    def __init__(self, df, feature, target):\n",
    "        self.df = df\n",
    "        self.feature = feature\n",
    "        self.target = target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Extract features and target\n",
    "        row = self.df.iloc[idx]\n",
    "        feature = row[self.feature] # Already consists tensors\n",
    "        target = torch.tensor(row[self.target], dtype=torch.long)\n",
    "        return feature, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_torch = DataFrameDataset(train_data, feature=feature_cat, target=\"enc_category\")\n",
    "val_torch = DataFrameDataset(val_data, feature=feature_cat, target=\"enc_category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 16\n",
    "# the batch size can be altered as need be, though 16 proves to be quite optimal already\n",
    "\n",
    "train_loader = DataLoader(train_torch, batch_size=batch, shuffle=True) \n",
    "val_loader = DataLoader(val_torch, batch_size=batch, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module): # creation of the Recurrent Neural Network (yes, an RNN - nt even an LSTM) for text classification\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True) # use of RNN function in nn module\n",
    "        self.fc = nn.Linear(hidden_size, output_size) # fully conected layer to map RNN output to number of output classes \n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device) # setting of hidden state params\n",
    "        out, _ = self.rnn(x, h0) # output\n",
    "        out = self.fc(out[:,-1,:]) # fully connected layer output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_size = 300 # vector w 300 elements\n",
    "hidden_size = 128 # in btw 300 and output + power of 2\n",
    "output_size = len(mapping)\n",
    "model = RNN(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss() # classification problem --> crossentropyloss type\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) # ADAM optimizer used --> allows for relatively 'large' learning rate\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [00:00<00:00, 182.71it/s]\n",
      "100%|██████████| 28/28 [00:00<00:00, 357.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "  Train Loss: 1.2835\n",
      "  Val Loss: 1.0526\n",
      "  Accuracy: 0.6315\n",
      "  Precision: 0.6586\n",
      "  F1 Score: 0.6293\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.53      0.83      0.65       102\n",
      "entertainment       0.79      0.48      0.60        77\n",
      "     politics       0.62      0.60      0.61        84\n",
      "        sport       0.70      0.58      0.63       102\n",
      "         tech       0.68      0.62      0.65        80\n",
      "\n",
      "     accuracy                           0.63       445\n",
      "    macro avg       0.66      0.62      0.63       445\n",
      " weighted avg       0.66      0.63      0.63       445\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [00:00<00:00, 150.65it/s]\n",
      "100%|██████████| 28/28 [00:00<00:00, 241.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:\n",
      "  Train Loss: 0.8601\n",
      "  Val Loss: 0.8904\n",
      "  Accuracy: 0.6989\n",
      "  Precision: 0.7050\n",
      "  F1 Score: 0.6977\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.65      0.81      0.72       102\n",
      "entertainment       0.77      0.62      0.69        77\n",
      "     politics       0.69      0.63      0.66        84\n",
      "        sport       0.74      0.67      0.70       102\n",
      "         tech       0.68      0.74      0.71        80\n",
      "\n",
      "     accuracy                           0.70       445\n",
      "    macro avg       0.71      0.69      0.70       445\n",
      " weighted avg       0.71      0.70      0.70       445\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [00:00<00:00, 128.82it/s]\n",
      "100%|██████████| 28/28 [00:00<00:00, 297.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:\n",
      "  Train Loss: 0.7090\n",
      "  Val Loss: 0.8286\n",
      "  Accuracy: 0.6966\n",
      "  Precision: 0.7000\n",
      "  F1 Score: 0.6945\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.67      0.81      0.74       102\n",
      "entertainment       0.68      0.70      0.69        77\n",
      "     politics       0.76      0.60      0.67        84\n",
      "        sport       0.70      0.73      0.71       102\n",
      "         tech       0.69      0.61      0.65        80\n",
      "\n",
      "     accuracy                           0.70       445\n",
      "    macro avg       0.70      0.69      0.69       445\n",
      " weighted avg       0.70      0.70      0.69       445\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [00:00<00:00, 102.20it/s]\n",
      "100%|██████████| 28/28 [00:00<00:00, 243.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:\n",
      "  Train Loss: 0.6235\n",
      "  Val Loss: 0.8102\n",
      "  Accuracy: 0.7034\n",
      "  Precision: 0.7041\n",
      "  F1 Score: 0.7029\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.72      0.74      0.73       102\n",
      "entertainment       0.67      0.75      0.71        77\n",
      "     politics       0.70      0.64      0.67        84\n",
      "        sport       0.72      0.72      0.72       102\n",
      "         tech       0.70      0.66      0.68        80\n",
      "\n",
      "     accuracy                           0.70       445\n",
      "    macro avg       0.70      0.70      0.70       445\n",
      " weighted avg       0.70      0.70      0.70       445\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [00:00<00:00, 133.21it/s]\n",
      "100%|██████████| 28/28 [00:00<00:00, 301.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:\n",
      "  Train Loss: 0.5675\n",
      "  Val Loss: 0.7952\n",
      "  Accuracy: 0.7169\n",
      "  Precision: 0.7201\n",
      "  F1 Score: 0.7163\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.69      0.79      0.74       102\n",
      "entertainment       0.72      0.73      0.72        77\n",
      "     politics       0.73      0.63      0.68        84\n",
      "        sport       0.78      0.70      0.74       102\n",
      "         tech       0.68      0.72      0.70        80\n",
      "\n",
      "     accuracy                           0.72       445\n",
      "    macro avg       0.72      0.71      0.71       445\n",
      " weighted avg       0.72      0.72      0.72       445\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [00:00<00:00, 134.50it/s]\n",
      "100%|██████████| 28/28 [00:00<00:00, 262.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:\n",
      "  Train Loss: 0.5275\n",
      "  Val Loss: 0.7941\n",
      "  Accuracy: 0.7011\n",
      "  Precision: 0.7032\n",
      "  F1 Score: 0.7005\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.68      0.79      0.73       102\n",
      "entertainment       0.74      0.68      0.71        77\n",
      "     politics       0.69      0.65      0.67        84\n",
      "        sport       0.74      0.70      0.72       102\n",
      "         tech       0.67      0.66      0.67        80\n",
      "\n",
      "     accuracy                           0.70       445\n",
      "    macro avg       0.70      0.70      0.70       445\n",
      " weighted avg       0.70      0.70      0.70       445\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [00:00<00:00, 158.17it/s]\n",
      "100%|██████████| 28/28 [00:00<00:00, 308.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10:\n",
      "  Train Loss: 0.4923\n",
      "  Val Loss: 0.7912\n",
      "  Accuracy: 0.7169\n",
      "  Precision: 0.7185\n",
      "  F1 Score: 0.7163\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.74      0.75      0.74       102\n",
      "entertainment       0.77      0.69      0.73        77\n",
      "     politics       0.69      0.69      0.69        84\n",
      "        sport       0.69      0.78      0.73       102\n",
      "         tech       0.71      0.65      0.68        80\n",
      "\n",
      "     accuracy                           0.72       445\n",
      "    macro avg       0.72      0.71      0.71       445\n",
      " weighted avg       0.72      0.72      0.72       445\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [00:00<00:00, 123.56it/s]\n",
      "100%|██████████| 28/28 [00:00<00:00, 322.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10:\n",
      "  Train Loss: 0.4628\n",
      "  Val Loss: 0.7962\n",
      "  Accuracy: 0.7011\n",
      "  Precision: 0.7046\n",
      "  F1 Score: 0.7015\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.76      0.71      0.73       102\n",
      "entertainment       0.76      0.65      0.70        77\n",
      "     politics       0.64      0.69      0.67        84\n",
      "        sport       0.70      0.75      0.73       102\n",
      "         tech       0.65      0.69      0.67        80\n",
      "\n",
      "     accuracy                           0.70       445\n",
      "    macro avg       0.70      0.70      0.70       445\n",
      " weighted avg       0.70      0.70      0.70       445\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [00:00<00:00, 140.43it/s]\n",
      "100%|██████████| 28/28 [00:00<00:00, 271.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10:\n",
      "  Train Loss: 0.4432\n",
      "  Val Loss: 0.8041\n",
      "  Accuracy: 0.7213\n",
      "  Precision: 0.7256\n",
      "  F1 Score: 0.7209\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.68      0.77      0.72       102\n",
      "entertainment       0.80      0.69      0.74        77\n",
      "     politics       0.74      0.64      0.69        84\n",
      "        sport       0.71      0.77      0.74       102\n",
      "         tech       0.72      0.70      0.71        80\n",
      "\n",
      "     accuracy                           0.72       445\n",
      "    macro avg       0.73      0.72      0.72       445\n",
      " weighted avg       0.73      0.72      0.72       445\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [00:00<00:00, 135.92it/s]\n",
      "100%|██████████| 28/28 [00:00<00:00, 279.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10:\n",
      "  Train Loss: 0.4313\n",
      "  Val Loss: 0.8387\n",
      "  Accuracy: 0.7034\n",
      "  Precision: 0.7128\n",
      "  F1 Score: 0.7029\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.66      0.79      0.72       102\n",
      "entertainment       0.64      0.77      0.70        77\n",
      "     politics       0.72      0.69      0.71        84\n",
      "        sport       0.81      0.64      0.71       102\n",
      "         tech       0.70      0.62      0.66        80\n",
      "\n",
      "     accuracy                           0.70       445\n",
      "    macro avg       0.71      0.70      0.70       445\n",
      " weighted avg       0.71      0.70      0.70       445\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    # Training Phase\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for features, target in tqdm(train_loader):\n",
    "        features, target = features.to(device), target.to(device)\n",
    "        features = features.unsqueeze(1)  # Adjust input for RNN\n",
    "        optimizer.zero_grad() # reset gradient to zero for each loop\n",
    "        output = model(features) # pass in my features into the model\n",
    "        loss = criterion(output, target) # using cross entropy loss WOO\n",
    "        loss.backward() # backprop \n",
    "        optimizer.step() # step taken\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval() # set model in evaluation phase\n",
    "    val_loss = 0\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, target in tqdm(val_loader):\n",
    "            features, target = features.to(device), target.to(device)\n",
    "            features = features.unsqueeze(1)\n",
    "            \n",
    "            output = model(features)\n",
    "            loss = criterion(output, target)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(output, dim=1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "\n",
    "    # Metrics Calculation \n",
    "    precision = precision_score(all_targets, all_predictions, average='weighted')\n",
    "    f1 = f1_score(all_targets, all_predictions, average='weighted')\n",
    "    accuracy = (torch.tensor(all_predictions) == torch.tensor(all_targets)).sum().item() / len(all_targets)\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  F1 Score: {f1:.4f}\")\n",
    "    print(classification_report(all_targets,all_predictions,target_names=mapping))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing(df,feature_name): # processing of future (test) dataframes for model suitability\n",
    "    feature = df[feature_name]\n",
    "    target = df[\"category\"]\n",
    "    \n",
    "    enc_feature = feature.apply(lambda x: torch.from_numpy(nlp(x.lower()).vector))\n",
    "    enc_target = target.map(mapping)\n",
    "    enc_df = pd.DataFrame({\"embed_feature\":enc_feature,\"enc_category\":enc_target})\n",
    "\n",
    "    torch_dataset = DataFrameDataset(enc_df, feature=\"embed_feature\", target=\"enc_category\")\n",
    "    torch_loader = DataLoader(torch_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "    return torch_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_test = processing(test_df,feature_cat.replace(\"embed\",\"\").replace(\"_\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7242\n",
      "F1 Score: 0.7186\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.69      0.73      0.70       102\n",
      "entertainment       0.66      0.81      0.73        77\n",
      "     politics       0.77      0.73      0.75        84\n",
      "        sport       0.76      0.63      0.69       102\n",
      "         tech       0.74      0.74      0.74        80\n",
      "\n",
      "     accuracy                           0.72       445\n",
      "    macro avg       0.72      0.72      0.72       445\n",
      " weighted avg       0.72      0.72      0.72       445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval() # Evaluation of the model on a test dataset\n",
    "\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad(): \n",
    "    for batch_idx, (features, targets) in enumerate(torch_test):\n",
    "        features = features.to(device)  \n",
    "        features = features.unsqueeze(1)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        logits = model(features)\n",
    "\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "        all_predictions.extend(predictions.cpu().numpy())  \n",
    "        all_targets.extend(targets.cpu().numpy()) \n",
    "\n",
    "precision = precision_score(all_targets, all_predictions, average='weighted')\n",
    "f1 = f1_score(all_targets, all_predictions, average='weighted')\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(classification_report(all_targets,all_predictions,target_names=mapping))\n",
    "# Quite good results - especially solely off titles and using just an RNN - even more impressive results can be attained \n",
    "# when the feature \"embed_content\" (embedded article text) is used for training and testing --> likely that an LSTM can \n",
    "# also result in improvements "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Topic Modelling\n",
    "\n",
    "It is useful to be able to classify articles into clear categories with which we can capture the interests of particular people / be able to pass proper decisions on - yet in the real worl, most data is unlabelled. More accurately, while a certain corpus of text may be accurately labelled (via crosschecking) by humans, newly generated articles may not - yielding problems such as the inability to consistently check if the preestablished classifier is able to perform well continuously on data that can change (even as its labels does not) / not reflecting novel categories well. This is where semi-supervised learning comes in; namely (in the case of NLP) [Topic Modelling](https://www.datacamp.com/tutorial/what-is-topic-modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_path,sep='\\t')\n",
    "r_state = 27\n",
    "\n",
    "train_df,test_df = train_test_split(df,train_size=0.8,random_state=r_state,stratify = df['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\",disable=[\"ner\",\"parser\"]) \n",
    "# The named-entity-recognition software and parser functions are disabled in order for increased speed during text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(sentence):\n",
    "    sentence = sentence.strip()\n",
    "    doc = nlp(sentence.lower())  # Lowercasing\n",
    "    processed_res = \" \".join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct and len(token.lemma_)>2])\n",
    "    return processed_res \n",
    "    \n",
    "def text_embedding(processed_res): \n",
    "    return torch.from_numpy(np.array([token.vector for token in nlp(processed_res)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"processed_content\"] = train_df[\"content\"].apply(text_preprocessing) \n",
    "# this aims to tokenise the text but not embed it directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['title_embed'] = train_df['title'].apply(lambda x:torch.from_numpy(np.array([token.vector for token in nlp(x)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(train_df,train_size=0.75,random_state=r_state,stratify=train_df['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenised_docs = list((train[\"processed_content\"]).apply(lambda x:x.split(' ')).values)\n",
    "# Topic Modelling Algorithms such as Latent Dirichlet Allocation (LDA) works off the analysis of tokens in numerous documents\n",
    "# rather than the embeddings of specific words/sentences or documents - hence; the matrix of various tokens are used to form \n",
    "# a Dictionary (in the case of LDA) for a corpus to be formed for the LDA model to be trained on. More information specifically\n",
    "# on LDA & how it works can be found here: https://towardsdatascience.com/latent-dirichlet-allocation-lda-9d1cd064ffa2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(tokenised_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in tokenised_docs] # this specific corpus is formed using the bag of words technique\n",
    "# term frequency - inverse document frequency (TF-IDF) can also be used, though for this specific task I found the bag of words\n",
    "# approach to be suitable in its regard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=5, passes=250, random_state=r_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison on train set\n",
    "predicted_topics = []\n",
    "for tokens in tokenised_docs:\n",
    "    bow = dictionary.doc2bow(tokens)\n",
    "    topic_distribution = lda_model.get_document_topics(bow, minimum_probability=0.0)\n",
    "    predicted_topic = max(topic_distribution, key=lambda x: x[1])[0]  # Topic with highest probability\n",
    "    predicted_topics.append(predicted_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category       predicted_cat\n",
       "business       4                91.830065\n",
       "               3                 3.267974\n",
       "               2                 2.941176\n",
       "               1                 1.633987\n",
       "               0                 0.326797\n",
       "entertainment  2                65.948276\n",
       "               0                25.000000\n",
       "               3                 5.603448\n",
       "               1                 1.724138\n",
       "               4                 1.724138\n",
       "politics       2                94.377510\n",
       "               4                 4.016064\n",
       "               1                 0.803213\n",
       "               0                 0.401606\n",
       "               3                 0.401606\n",
       "sport          1                98.697068\n",
       "               0                 0.977199\n",
       "               4                 0.325733\n",
       "tech           3                83.817427\n",
       "               2                 5.394191\n",
       "               1                 4.564315\n",
       "               0                 3.319502\n",
       "               4                 2.904564\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison = pd.concat([train[\"category\"].reset_index().drop(\"index\",axis=1),pd.Series(predicted_topics,name='predicted_cat')],axis=1)\n",
    "comparison.groupby(\"category\").value_counts(normalize=True)*100 # the information presented below has been more neatly conveyed\n",
    "# in cells further below - where my conclusion has also been outlined. See if you can come to the same realisation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tokenised_docs = list((val[\"processed_content\"]).apply(lambda x:x.split(' ')).values)\n",
    "\n",
    "predicted_topics = []\n",
    "for tokens in val_tokenised_docs:\n",
    "    bow = dictionary.doc2bow(tokens)\n",
    "    topic_distribution = lda_model.get_document_topics(bow, minimum_probability=0.0)\n",
    "    predicted_topic = max(topic_distribution, key=lambda x: x[1])[0]  # Topic with highest probability\n",
    "    predicted_topics.append(predicted_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>predicted_cat</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>entertainment</th>\n",
       "      <td>18.181818</td>\n",
       "      <td>1.298701</td>\n",
       "      <td>70.129870</td>\n",
       "      <td>9.090909</td>\n",
       "      <td>1.298701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sport</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>99.019608</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tech</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>91.250000</td>\n",
       "      <td>1.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>business</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>3.921569</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>95.098039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>politics</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98.809524</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.190476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "predicted_cat          0          1          2          3          4\n",
       "category                                                            \n",
       "entertainment  18.181818   1.298701  70.129870   9.090909   1.298701\n",
       "sport           0.000000  99.019608   0.980392   0.000000   0.000000\n",
       "tech            0.000000   1.250000   6.250000  91.250000   1.250000\n",
       "business        0.000000   0.980392   3.921569   0.000000  95.098039\n",
       "politics        0.000000   0.000000  98.809524   0.000000   1.190476"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison = pd.concat([val[\"category\"].reset_index().drop(\"index\",axis=1),pd.Series(predicted_topics,name='predicted_cat')],axis=1)\n",
    "comparison[\"count\"]=1 \n",
    "pf = pd.crosstab(index=comparison[\"category\"],columns=comparison[\"predicted_cat\"],values=comparison[\"count\"],aggfunc=\"sum\",normalize=\"index\")*100\n",
    "pf.sort_values(by=[0,1,2,3,4],ascending=False)\n",
    "# There appears to be a pretty good split of the actual categories with the predicted (unnamed) categories formed by the LDA model\n",
    "# Articles labelled as 'sport' are lumped under the model's category '1' \n",
    "# Articles labelled as 'tech' are primarily lumped under the model's category '3' and so on\n",
    "\n",
    "# Indeed, the main problem appears to be the overlap between the entertainment and political articles which are primarily\n",
    "# clustered together under '2' by the LDA model - though more of entertainment is outlied into '0' \n",
    "\n",
    "# As LDA doesn't hold any specific semantic regard for the words themselves - working solely off of tokenised instanced of \n",
    "# preprocessed words - it naturally doesn't look out for the meaning necessarily but rather instances of frequency and likelihood\n",
    "# This can point to a similarity in the construction ntertainment articles and politics - indeed politicians today do appear to be \n",
    "# attaining that 'celebrity' like status - which is helpful in providing new perspectives; though not without its flaws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_cat\n",
      "0    entertainment\n",
      "1            sport\n",
      "2         politics\n",
      "3             tech\n",
      "4         business\n",
      "dtype: object\n",
      "\n",
      "category\n",
      "business         4\n",
      "entertainment    2\n",
      "politics         2\n",
      "sport            1\n",
      "tech             3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(pf.idxmax()) # Another illustration of the above table\n",
    "print()\n",
    "print(pf.T.idxmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_mapping = {k:i for i,k in enumerate(train[\"category\"].unique())} \n",
    "train[\"enc_category\"] = train[\"category\"].map(cat_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_content,sample_cats = train[\"processed_content\"].values ,train[\"enc_category\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-22 22:39:55,083 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4edca83fd3e94b40a60eb3578b074fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-22 22:42:00,405 - BERTopic - Embedding - Completed ✓\n",
      "2024-12-22 22:42:00,406 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-12-22 22:42:11,403 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-12-22 22:42:11,403 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-12-22 22:42:11,503 - BERTopic - Cluster - Completed ✓\n",
      "2024-12-22 22:42:11,503 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-12-22 22:42:11,767 - BERTopic - Representation - Completed ✓\n",
      "2024-12-22 22:42:11,767 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2024-12-22 22:42:11,767 - BERTopic - Topic reduction - Reduced number of topics from 5 to 5\n"
     ]
    }
   ],
   "source": [
    "topic_model = BERTopic(verbose=True, nr_topics = 5,min_topic_size=150)\n",
    "topics,probs = topic_model.fit_transform(sample_content,y = sample_cats)\n",
    "\n",
    "# Apart from LDA, BERTopic is another unsupervised Machine Learning Algorithm that can identify common topics in various articles\n",
    "# through the analysis of the tokenised documents of these articles. Unlike LDA however, it is not solely reliant on individual tokens\n",
    "# nor statistical probabilities - but is also able to capture the meaning of articles in its topic modelling process - yielding \n",
    "# far better results in identifying common topics without requiring the number of topics to be explicitly defined (like in LDA); though\n",
    "# this can be done as shown above to acquire na outcome efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>249</td>\n",
       "      <td>0_say_labour_party_election</td>\n",
       "      <td>[say, labour, party, election, government, bla...</td>\n",
       "      <td>[tony blair launch attack conservative spendin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>233</td>\n",
       "      <td>1_film_award_say_star</td>\n",
       "      <td>[film, award, say, star, year, good, music, in...</td>\n",
       "      <td>[hollywood star bring touch glamour london sat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>307</td>\n",
       "      <td>2_win_play_game_say</td>\n",
       "      <td>[win, play, game, say, player, england, year, ...</td>\n",
       "      <td>[double olympic champion kelly holmes good com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>306</td>\n",
       "      <td>3_say_year_company_firm</td>\n",
       "      <td>[say, year, company, firm, market, rise, bank,...</td>\n",
       "      <td>[price home rise seasonally adjust 0.5 februar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>240</td>\n",
       "      <td>4_say_people_technology_game</td>\n",
       "      <td>[say, people, technology, game, mobile, servic...</td>\n",
       "      <td>[mobile phone enjoy boom time sale accord rese...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                          Name  \\\n",
       "0      0    249   0_say_labour_party_election   \n",
       "1      1    233         1_film_award_say_star   \n",
       "2      2    307           2_win_play_game_say   \n",
       "3      3    306       3_say_year_company_firm   \n",
       "4      4    240  4_say_people_technology_game   \n",
       "\n",
       "                                      Representation  \\\n",
       "0  [say, labour, party, election, government, bla...   \n",
       "1  [film, award, say, star, year, good, music, in...   \n",
       "2  [win, play, game, say, player, england, year, ...   \n",
       "3  [say, year, company, firm, market, rise, bank,...   \n",
       "4  [say, people, technology, game, mobile, servic...   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [tony blair launch attack conservative spendin...  \n",
       "1  [hollywood star bring touch glamour london sat...  \n",
       "2  [double olympic champion kelly holmes good com...  \n",
       "3  [price home rise seasonally adjust 0.5 februar...  \n",
       "4  [mobile phone enjoy boom time sale accord rese...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic_info() # the Names of each topic roughly reflect the documents within quite well\n",
    "# Topic 0 likely being political in nature, 1 being film/ entertainment based, 3 being business related and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>predicted_cat</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>business</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.326797</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.673203</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entertainment</th>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>politics</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sport</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tech</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.414938</td>\n",
       "      <td>99.585062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "predicted_cat      0           1      2          3          4\n",
       "category                                                     \n",
       "business         0.0    0.326797    0.0  99.673203   0.000000\n",
       "entertainment    0.0  100.000000    0.0   0.000000   0.000000\n",
       "politics       100.0    0.000000    0.0   0.000000   0.000000\n",
       "sport            0.0    0.000000  100.0   0.000000   0.000000\n",
       "tech             0.0    0.000000    0.0   0.414938  99.585062"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison = pd.concat([train[\"category\"].reset_index().drop(\"index\",axis=1),pd.Series(topics,name='predicted_cat')],axis=1)\n",
    "comparison[\"count\"]=1 \n",
    "pf = pd.crosstab(index=comparison[\"category\"],columns=comparison[\"predicted_cat\"],values=comparison[\"count\"],aggfunc=\"sum\",normalize=\"index\")*100\n",
    "pf # We can observe the staggering improvement in how BERTopic segregated the various topics into its categories compared to LDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_cat\n",
      "0         politics\n",
      "1    entertainment\n",
      "2            sport\n",
      "3         business\n",
      "4             tech\n",
      "dtype: object\n",
      "\n",
      "category\n",
      "business         3\n",
      "entertainment    1\n",
      "politics         0\n",
      "sport            2\n",
      "tech             4\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(pf.idxmax())\n",
    "print()\n",
    "print(pf.T.idxmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "val[\"enc_category\"] = val[\"category\"].map(cat_mapping) # the validation process is thus carried out below, to achieve similar results\n",
    "val_content,val_cats = val[\"processed_content\"].values ,val[\"enc_category\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c99c4f982124670bc7bb6f882d55a3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-22 22:42:52,530 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2024-12-22 22:42:58,194 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-12-22 22:42:58,196 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2024-12-22 22:42:58,226 - BERTopic - Cluster - Completed ✓\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>predicted_cat</th>\n",
       "      <th>-1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>business</th>\n",
       "      <td>0.980392</td>\n",
       "      <td>3.921569</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>92.156863</td>\n",
       "      <td>2.941176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entertainment</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.298701</td>\n",
       "      <td>92.207792</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.298701</td>\n",
       "      <td>5.194805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>politics</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>98.809524</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.190476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sport</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>99.019608</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tech</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>95.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "predicted_cat        -1          0          1          2          3          4\n",
       "category                                                                      \n",
       "business       0.980392   3.921569   0.000000   0.000000  92.156863   2.941176\n",
       "entertainment  0.000000   1.298701  92.207792   0.000000   1.298701   5.194805\n",
       "politics       0.000000  98.809524   0.000000   0.000000   0.000000   1.190476\n",
       "sport          0.000000   0.980392   0.000000  99.019608   0.000000   0.000000\n",
       "tech           0.000000   1.250000   0.000000   0.000000   3.750000  95.000000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics,probs = topic_model.transform(val_content)\n",
    "\n",
    "comparison = pd.concat([val[\"category\"].reset_index().drop(\"index\",axis=1),pd.Series(topics,name='predicted_cat')],axis=1)\n",
    "comparison[\"count\"]=1 \n",
    "pf = pd.crosstab(index=comparison[\"category\"],columns=comparison[\"predicted_cat\"],values=comparison[\"count\"],aggfunc=\"sum\",normalize=\"index\")*100\n",
    "pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_cat\n",
      "-1         business\n",
      " 0         politics\n",
      " 1    entertainment\n",
      " 2            sport\n",
      " 3         business\n",
      " 4             tech\n",
      "dtype: object\n",
      "\n",
      "category\n",
      "business         3\n",
      "entertainment    1\n",
      "politics         0\n",
      "sport            2\n",
      "tech             4\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(pf.idxmax())\n",
    "print()\n",
    "print(pf.T.idxmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"processed_content\"] = test_df[\"content\"].apply(text_preprocessing) # likewise for the test set\n",
    "test_df[\"enc_category\"] = test_df[\"category\"].map(cat_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_content,test_cats = test_df[\"processed_content\"].values ,test_df[\"enc_category\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a6e7bcbeea4a66ba5727628e069660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-22 22:43:51,070 - BERTopic - Dimensionality - Reducing dimensionality of input embeddings.\n",
      "2024-12-22 22:43:52,272 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-12-22 22:43:52,272 - BERTopic - Clustering - Approximating new points with `hdbscan_model`\n",
      "2024-12-22 22:43:52,293 - BERTopic - Cluster - Completed ✓\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>predicted_cat</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>business</th>\n",
       "      <td>3.921569</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.960784</td>\n",
       "      <td>89.215686</td>\n",
       "      <td>4.901961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entertainment</th>\n",
       "      <td>2.597403</td>\n",
       "      <td>92.207792</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.298701</td>\n",
       "      <td>3.896104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>politics</th>\n",
       "      <td>95.238095</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.190476</td>\n",
       "      <td>2.380952</td>\n",
       "      <td>1.190476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sport</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>99.019608</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tech</th>\n",
       "      <td>3.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>93.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "predicted_cat          0          1          2          3          4\n",
       "category                                                            \n",
       "business        3.921569   0.000000   1.960784  89.215686   4.901961\n",
       "entertainment   2.597403  92.207792   0.000000   1.298701   3.896104\n",
       "politics       95.238095   0.000000   1.190476   2.380952   1.190476\n",
       "sport           0.000000   0.000000  99.019608   0.980392   0.000000\n",
       "tech            3.750000   0.000000   0.000000   2.500000  93.750000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics,probs = topic_model.transform(test_content)\n",
    "\n",
    "comparison = pd.concat([test_df[\"category\"].reset_index().drop(\"index\",axis=1),pd.Series(topics,name='predicted_cat')],axis=1)\n",
    "comparison[\"count\"]=1 \n",
    "pf = pd.crosstab(index=comparison[\"category\"],columns=comparison[\"predicted_cat\"],values=comparison[\"count\"],aggfunc=\"sum\",normalize=\"index\")*100\n",
    "pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_cat\n",
      "0         politics\n",
      "1    entertainment\n",
      "2            sport\n",
      "3         business\n",
      "4             tech\n",
      "dtype: object\n",
      "\n",
      "category\n",
      "business         3\n",
      "entertainment    1\n",
      "politics         0\n",
      "sport            2\n",
      "tech             4\n",
      "dtype: int32\n"
     ]
    }
   ],
   "source": [
    "print(pf.idxmax())\n",
    "print()\n",
    "print(pf.T.idxmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>12</td>\n",
       "      <td>-1_stern_viacom_season_say</td>\n",
       "      <td>[stern, viacom, season, say, wrist, hoddle, wo...</td>\n",
       "      <td>[medium giant viacom pay 3.5 1.8 end investiga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>287</td>\n",
       "      <td>0_say_year_company_firm</td>\n",
       "      <td>[say, year, company, firm, rise, market, bank,...</td>\n",
       "      <td>[house price increase 1.1 december monthly ris...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>262</td>\n",
       "      <td>1_say_labour_party_election</td>\n",
       "      <td>[say, labour, party, election, government, bla...</td>\n",
       "      <td>[tony blair launch attack conservative spendin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>246</td>\n",
       "      <td>2_say_people_technology_mobile</td>\n",
       "      <td>[say, people, technology, mobile, service, gam...</td>\n",
       "      <td>[mobile phone enjoy boom time sale accord rese...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>136</td>\n",
       "      <td>3_film_award_star_good</td>\n",
       "      <td>[film, award, star, good, actor, oscar, year, ...</td>\n",
       "      <td>[martin scorsese aviator win good film oscar a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>112</td>\n",
       "      <td>4_club_united_liverpool_goal</td>\n",
       "      <td>[club, united, liverpool, goal, chelsea, game,...</td>\n",
       "      <td>[arsene wenger step feud sir alex ferguson cla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>92</td>\n",
       "      <td>5_england_rugby_ireland_nation</td>\n",
       "      <td>[england, rugby, ireland, nation, game, win, c...</td>\n",
       "      <td>[lansdowne road dublin sunday february 1500 gm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>79</td>\n",
       "      <td>6_music_band_album_song</td>\n",
       "      <td>[music, band, album, song, number, record, say...</td>\n",
       "      <td>[memory soul legend ray charles dominate music...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>57</td>\n",
       "      <td>7_roddick_win_match_play</td>\n",
       "      <td>[roddick, win, match, play, set, open, seed, n...</td>\n",
       "      <td>[andre agassi erratic display edge fourth roun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>52</td>\n",
       "      <td>8_olympic_race_win_athlete</td>\n",
       "      <td>[olympic, race, win, athlete, athens, indoor, ...</td>\n",
       "      <td>[britain jason gardener shake upset stomach wi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                            Name  \\\n",
       "0     -1     12      -1_stern_viacom_season_say   \n",
       "1      0    287         0_say_year_company_firm   \n",
       "2      1    262     1_say_labour_party_election   \n",
       "3      2    246  2_say_people_technology_mobile   \n",
       "4      3    136          3_film_award_star_good   \n",
       "5      4    112    4_club_united_liverpool_goal   \n",
       "6      5     92  5_england_rugby_ireland_nation   \n",
       "7      6     79         6_music_band_album_song   \n",
       "8      7     57        7_roddick_win_match_play   \n",
       "9      8     52      8_olympic_race_win_athlete   \n",
       "\n",
       "                                      Representation  \\\n",
       "0  [stern, viacom, season, say, wrist, hoddle, wo...   \n",
       "1  [say, year, company, firm, rise, market, bank,...   \n",
       "2  [say, labour, party, election, government, bla...   \n",
       "3  [say, people, technology, mobile, service, gam...   \n",
       "4  [film, award, star, good, actor, oscar, year, ...   \n",
       "5  [club, united, liverpool, goal, chelsea, game,...   \n",
       "6  [england, rugby, ireland, nation, game, win, c...   \n",
       "7  [music, band, album, song, number, record, say...   \n",
       "8  [roddick, win, match, play, set, open, seed, n...   \n",
       "9  [olympic, race, win, athlete, athens, indoor, ...   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [medium giant viacom pay 3.5 1.8 end investiga...  \n",
       "1  [house price increase 1.1 december monthly ris...  \n",
       "2  [tony blair launch attack conservative spendin...  \n",
       "3  [mobile phone enjoy boom time sale accord rese...  \n",
       "4  [martin scorsese aviator win good film oscar a...  \n",
       "5  [arsene wenger step feud sir alex ferguson cla...  \n",
       "6  [lansdowne road dublin sunday february 1500 gm...  \n",
       "7  [memory soul legend ray charles dominate music...  \n",
       "8  [andre agassi erratic display edge fourth roun...  \n",
       "9  [britain jason gardener shake upset stomach wi...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model = BERTopic(min_topic_size=20) \n",
    "topics,probs = topic_model.fit_transform(sample_content)\n",
    "topic_info = topic_model.get_topic_info()\n",
    "topic_info\n",
    "\n",
    "# indeed, BERTopic is useful since it does not demand a specific number of topics to be outlined from the get go - \n",
    "# so long as a rough estimate of the minimum number of topics can be provided - BERTopic will automatically cluster various \n",
    "# topics together and outline the result in a datafrae=me as shown below. This can be more clearly visualised in a \n",
    "# hierarchy of topics - showing the links between topics as well as outlining how defined 'catgeorised' topics like sport\n",
    "# or entertainment can be subdivided by particular regions/ players/ genres etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 234.69it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hoverinfo": "text",
         "marker": {
          "color": "rgb(61,153,112)"
         },
         "mode": "lines",
         "text": [
          "england_rugby_ireland_nation_game",
          "",
          "",
          "club_united_liverpool_goal_chelsea"
         ],
         "type": "scatter",
         "x": [
          0,
          0.5094165930924499,
          0.5094165930924499,
          0
         ],
         "xaxis": "x",
         "y": [
          -25,
          -25,
          -35,
          -35
         ],
         "yaxis": "y"
        },
        {
         "hoverinfo": "text",
         "marker": {
          "color": "rgb(61,153,112)"
         },
         "mode": "lines",
         "text": [
          "roddick_win_match_play_set",
          "",
          "",
          "england_game_club_player_play"
         ],
         "type": "scatter",
         "x": [
          0,
          0.615212240027177,
          0.615212240027177,
          0.5094165930924499
         ],
         "xaxis": "x",
         "y": [
          -15,
          -15,
          -30,
          -30
         ],
         "yaxis": "y"
        },
        {
         "hoverinfo": "text",
         "marker": {
          "color": "rgb(61,153,112)"
         },
         "mode": "lines",
         "text": [
          "olympic_race_win_athlete_athens",
          "",
          "",
          "play_game_win_player_england"
         ],
         "type": "scatter",
         "x": [
          0,
          0.6722994235581738,
          0.6722994235581738,
          0.615212240027177
         ],
         "xaxis": "x",
         "y": [
          -5,
          -5,
          -22.5,
          -22.5
         ],
         "yaxis": "y"
        },
        {
         "hoverinfo": "text",
         "marker": {
          "color": "rgb(255,65,54)"
         },
         "mode": "lines",
         "text": [
          "say_labour_party_election_government",
          "",
          "",
          "say_year_company_firm_rise"
         ],
         "type": "scatter",
         "x": [
          0,
          0.487366266969226,
          0.487366266969226,
          0
         ],
         "xaxis": "x",
         "y": [
          -45,
          -45,
          -55,
          -55
         ],
         "yaxis": "y"
        },
        {
         "hoverinfo": "text",
         "marker": {
          "color": "rgb(255,65,54)"
         },
         "mode": "lines",
         "text": [
          "say_government_year_labour_party",
          "",
          "",
          "say_people_technology_mobile_service"
         ],
         "type": "scatter",
         "x": [
          0.487366266969226,
          0.5232199078860851,
          0.5232199078860851,
          0
         ],
         "xaxis": "x",
         "y": [
          -50,
          -50,
          -65,
          -65
         ],
         "yaxis": "y"
        },
        {
         "hoverinfo": "text",
         "marker": {
          "color": "rgb(255,65,54)"
         },
         "mode": "lines",
         "text": [
          "music_band_album_song_number",
          "",
          "",
          "film_award_star_good_actor"
         ],
         "type": "scatter",
         "x": [
          0,
          0.5722685533978418,
          0.5722685533978418,
          0
         ],
         "xaxis": "x",
         "y": [
          -75,
          -75,
          -85,
          -85
         ],
         "yaxis": "y"
        },
        {
         "hoverinfo": "text",
         "marker": {
          "color": "rgb(255,65,54)"
         },
         "mode": "lines",
         "text": [
          "say_people_year_new_government",
          "",
          "",
          "film_award_star_good_year"
         ],
         "type": "scatter",
         "x": [
          0.5232199078860851,
          0.8072616773010255,
          0.8072616773010255,
          0.5722685533978418
         ],
         "xaxis": "x",
         "y": [
          -57.5,
          -57.5,
          -80,
          -80
         ],
         "yaxis": "y"
        },
        {
         "hoverinfo": "text",
         "marker": {
          "color": "rgb(0,116,217)"
         },
         "mode": "lines",
         "text": [
          "win_play_game_say_player",
          "",
          "",
          "say_year_people_new_film"
         ],
         "type": "scatter",
         "x": [
          0.6722994235581738,
          1.0031918526735832,
          1.0031918526735832,
          0.8072616773010255
         ],
         "xaxis": "x",
         "y": [
          -13.75,
          -13.75,
          -68.75,
          -68.75
         ],
         "yaxis": "y"
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "say_government_year_labour_party",
          "say_people_year_new_government",
          "win_play_game_say_player"
         ],
         "marker": {
          "color": "black"
         },
         "mode": "markers",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0.487366266969226,
          0.5232199078860851,
          0.6722994235581738
         ],
         "y": [
          -50,
          -57.5,
          -13.75
         ]
        },
        {
         "hoverinfo": "text",
         "hovertext": [
          "england_game_club_player_play",
          "play_game_win_player_england",
          "film_award_star_good_year",
          "say_year_people_new_film"
         ],
         "marker": {
          "color": "black"
         },
         "mode": "markers",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0.5094165930924499,
          0.615212240027177,
          0.5722685533978418,
          0.8072616773010255
         ],
         "y": [
          -30,
          -22.5,
          -80,
          -68.75
         ]
        }
       ],
       "layout": {
        "autosize": false,
        "height": 335,
        "hoverlabel": {
         "bgcolor": "white",
         "font": {
          "family": "Rockwell",
          "size": 16
         }
        },
        "hovermode": "closest",
        "plot_bgcolor": "#ECEFF1",
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "font": {
          "color": "Black",
          "size": 22
         },
         "text": "<b>Hierarchical Clustering</b>",
         "x": 0.5,
         "xanchor": "center",
         "yanchor": "top"
        },
        "width": 1000,
        "xaxis": {
         "mirror": "allticks",
         "rangemode": "tozero",
         "showgrid": false,
         "showline": true,
         "showticklabels": true,
         "ticks": "outside",
         "type": "linear",
         "zeroline": false
        },
        "yaxis": {
         "mirror": "allticks",
         "range": [
          -90,
          0
         ],
         "rangemode": "tozero",
         "showgrid": false,
         "showline": true,
         "showticklabels": true,
         "tickmode": "array",
         "ticks": "outside",
         "ticktext": [
          "8_olympic_race_win",
          "7_roddick_win_match",
          "5_england_rugby_ireland",
          "4_club_united_liverpool",
          "1_say_labour_party",
          "0_say_year_company",
          "2_say_people_technology",
          "6_music_band_album",
          "3_film_award_star"
         ],
         "tickvals": [
          -5,
          -15,
          -25,
          -35,
          -45,
          -55,
          -65,
          -75,
          -85
         ],
         "type": "linear",
         "zeroline": false
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hierarchical_topics = topic_model.hierarchical_topics(sample_content)\n",
    "topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)\n",
    "\n",
    "# For example, in sports, we can see that our articles can be split across Olympic events and also those in England/ the UK\n",
    "# Given that this is a BBC dataset - it does make sense to have sufficient articles related to sports in the UK to warrant \n",
    "# such a number of topics arising\n",
    "\n",
    "# We can also see links between technology and business as well as politics - pointing to the intrinsic nature of how \n",
    "# technology is at the forefront of many businesses today and indeed how governmnets are aware of their impacts - certainly\n",
    "# more than sports. Together with LDA, topic modelling techniques can provide various insights into categories that may already\n",
    "# be defined based on how they are constructed with regards to particular phrases that are used to how these categories are related\n",
    "# to one another and how they may be further subdivided to entice particular audiences well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Generation\n",
    "\n",
    "As shown above - topic modelling can prove to be a useful part of NLP in clustering articles of a related nature together. Particularly with regards to BERTopic, this can allow for useful, readable information pertaining to the class of articles to be quickly derived from unlabelled articles. However, it is quite clear that labels such as `'film_award_star'` and `'say_people_technology'`, while understandable, isn't immediately appealing for people to read and make sense of on the fly. This is likewise the case for extremely long articles with no clear headers about them. \n",
    "\n",
    "What is required is a good summarisation - a good TITLE that can capture people's attention. Such text ought to be generated quickly and reflect the tones of categories/ articles sufficiently well. Multiple pretrained Sequence2Sequence models, as well as transformers, have been developed for this - and given the great performance of BERTopic as seen previously - a similar approach shall be taken to see if ML Algos can generated as good titles for articles as we can ourselves!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"czearing/article-title-generator\" \n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# information of this model can be found on huggingface via https://huggingface.co/czearing/article-title-generator\n",
    "# This model in particular was selected owing to its similarity with the task of title generation I aimed to accomplished\n",
    "# The model is loaded and utilised directly - without any prior training - owing to hardware limitations, yet owing to its \n",
    "# similar mission it had to accomplished when trained initially, its results appear to be fairly impressive indeed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_path,sep='\\t')\n",
    "r_state = 27\n",
    "train_df,test_df = train_test_split(df,train_size=0.8,random_state=r_state,stratify = df['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2225 entries, 0 to 2224\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   category  2225 non-null   object\n",
      " 1   filename  2225 non-null   object\n",
      " 2   title     2225 non-null   object\n",
      " 3   content   2225 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 69.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(train_df,train_size=0.75,random_state=r_state,stratify=train_df['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title\n",
       "5    618\n",
       "6    384\n",
       "4    229\n",
       "7     78\n",
       "3     23\n",
       "8      2\n",
       "9      1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"title\"].str.split(\" \").apply(lambda x:len(x)).value_counts() # The length of the title is analysed to give a rough approximation \n",
    "# to the maximum length of titles which the model should not exceed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_title_len = 15 # The max title length (in the model) is based off tokens, not words. Given how sites such as\n",
    "# https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them suggest the approximation of using \n",
    "# 4 tokens for 3 words, and the fact that the sole splitting of spaces to count words isn't exactly the best metric - to err\n",
    "# on the side of caution - I have placed the maximum title length (of tokens) to be 15 instead of 12. This can be amended as \n",
    "# one wants - but ultimately proves to be sufficient in my opinion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion of each Pandas DataFrame to a Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(train)\n",
    "val_dataset = Dataset.from_pandas(val)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0db76236d284704ae9c4c1f2f0946e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1335 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39d716c37f5f4acc9e22ff7d49c8c14a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/445 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25cbbc753cbf4d39a8cbbc862277e172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/445 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples,feature_col=\"content\",target_col=\"title\"):\n",
    "    # Tokenises the article as the input and the title as the target\n",
    "    inputs = tokenizer(examples[feature_col], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    targets = tokenizer(examples[\"title\"], padding=\"max_length\", truncation=True, max_length=max_title_len)\n",
    "    inputs['labels'] = targets['input_ids']  # Set titles as labels\n",
    "    return inputs\n",
    "\n",
    "# Tokenise all datasets (train, val, test)\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_shuffle = test_dataset.shuffle(seed=r_state)\n",
    "size_shuffle = 25 # the number of titles to be generated for and compared against (advised to keep the number constrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate titles for each article in the validation set\n",
    "generated_titles = []\n",
    "\n",
    "for article in rand_shuffle['input_ids'][:size_shuffle]:\n",
    "    generated_ids = model.generate(torch.from_numpy(np.array(article)).unsqueeze(0), num_beams=4, min_length = 3\n",
    "        ,max_length=max_title_len, early_stopping=True)\n",
    "    generated_title = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    generated_titles.append(generated_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True) \n",
    "\n",
    "# The rouge score is a metric commonly used (alongside the BLEU score) for the evaluation of machine text generation and summarisation\n",
    "# It has been preferred over the BLEU score owing to the latter's preference for machine translation tasks\n",
    "\n",
    "# https://medium.com/nlplanet/two-minutes-nlp-learn-the-rouge-metric-by-examples-f179cc285499"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(columns = [\"Actl_Title\",\"Gen_Title\",\"f_measure\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the generated titles alongside the original articles\n",
    "for actl_title, pred_title in zip(rand_shuffle['title'][:size_shuffle], generated_titles):\n",
    "    score = scorer.score(actl_title,pred_title)\n",
    "    res.loc[len(res)] = [actl_title,pred_title,score['rougeL'].fmeasure]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actl_Title</th>\n",
       "      <th>Gen_Title</th>\n",
       "      <th>f_measure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Clarke faces ID cards rebellion</td>\n",
       "      <td>Home Secretary Charles Clarke faces backbench rebellion over ID cards bill</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Blair buys copies of new Band Aid</td>\n",
       "      <td>Prime Minister Tony Blair bought two copies of Band Aid 20 in Edinburgh</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Women MPs reveal sexist taunts</td>\n",
       "      <td>Women MPs endure \"shocking\" levels of sexist abuse</td>\n",
       "      <td>0.461538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>UK troops on Ivory Coast standby</td>\n",
       "      <td>British troops on standby to help evacuate British citizens from Ivory Coast</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Beckham relief as Real go through</td>\n",
       "      <td>David Beckham expresses relief at Real Madrid's passage to Champions</td>\n",
       "      <td>0.352941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Collins to compete in Birmingham</td>\n",
       "      <td>World and Commonwealth 100m champion Kim Collins will compete in the 60m at</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Melzer shocks Agassi</td>\n",
       "      <td>Jurgen Melzer beat Andre Agassi 6-3 6-1</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Jamelia's return to the top</td>\n",
       "      <td>Jamelia Davis reveals why she's still trying to make it in</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Mobiles rack up 20 years of use</td>\n",
       "      <td>20 years since the first mobile phone call</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Security warning over 'FBI virus'</td>\n",
       "      <td>Emails purporting to be from the FBI contain a computer virus</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>'Few ready' for information act</td>\n",
       "      <td>Thousands of public bodies are ill-prepared for the Freedom of Information</td>\n",
       "      <td>0.235294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gadget market 'to grow in 2005'</td>\n",
       "      <td>The explosion in consumer technology is to continue into 2005, delegates at the</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Worcester v Sale (Fri)</td>\n",
       "      <td>Sale vs Sale vs Sale vs Sale</td>\n",
       "      <td>0.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mobile multimedia slow to catch on</td>\n",
       "      <td>Why people aren't sending multimedia messages</td>\n",
       "      <td>0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Beijingers fume over parking fees</td>\n",
       "      <td>Beijing officials to look at reorganising car parking charges</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kenya lift Chepkemei's suspension</td>\n",
       "      <td>Athletics Kenya reverses ban on marathon runner Susan Chepkeme</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Internet boom for gift shopping</td>\n",
       "      <td>Cyberspace is becoming a very popular destination for Christmas shoppers</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Plan to give elderly care control</td>\n",
       "      <td>The government shuns calls for free long-term care</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Music mogul Fuller sells company</td>\n",
       "      <td>Pop Idol supremo Simon Fuller has sold his 19 Entertainment</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Child access law shake-up planned</td>\n",
       "      <td>Lord Falconer outlines plans for electronic tagging of uncooperative</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>REM announce new Glasgow concert</td>\n",
       "      <td>REM to play for 10,000 Scottish fans in rescheduled gig</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Ink helps drive democracy in Asia</td>\n",
       "      <td>The Kyrgyz Republic is using invisible ink and ultraviolet readers</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Asian banks halt dollar's slide</td>\n",
       "      <td>Dollar regained some lost ground against most major currencies on Wednesday ...</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Digital UK driven by net and TV</td>\n",
       "      <td>The UK is the fourth most digitally-savvy nation in Europe</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Blair 'up for it' ahead of poll</td>\n",
       "      <td>Tony Blair says his personal standing will be an issue in the general election</td>\n",
       "      <td>0.095238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Actl_Title  \\\n",
       "8      Clarke faces ID cards rebellion   \n",
       "23   Blair buys copies of new Band Aid   \n",
       "4       Women MPs reveal sexist taunts   \n",
       "18    UK troops on Ivory Coast standby   \n",
       "17   Beckham relief as Real go through   \n",
       "5     Collins to compete in Birmingham   \n",
       "19                Melzer shocks Agassi   \n",
       "15         Jamelia's return to the top   \n",
       "22     Mobiles rack up 20 years of use   \n",
       "14   Security warning over 'FBI virus'   \n",
       "21     'Few ready' for information act   \n",
       "0      Gadget market 'to grow in 2005'   \n",
       "20              Worcester v Sale (Fri)   \n",
       "6   Mobile multimedia slow to catch on   \n",
       "9    Beijingers fume over parking fees   \n",
       "3    Kenya lift Chepkemei's suspension   \n",
       "1      Internet boom for gift shopping   \n",
       "11   Plan to give elderly care control   \n",
       "7     Music mogul Fuller sells company   \n",
       "2    Child access law shake-up planned   \n",
       "16    REM announce new Glasgow concert   \n",
       "12   Ink helps drive democracy in Asia   \n",
       "10     Asian banks halt dollar's slide   \n",
       "24     Digital UK driven by net and TV   \n",
       "13     Blair 'up for it' ahead of poll   \n",
       "\n",
       "                                                                          Gen_Title  \\\n",
       "8        Home Secretary Charles Clarke faces backbench rebellion over ID cards bill   \n",
       "23          Prime Minister Tony Blair bought two copies of Band Aid 20 in Edinburgh   \n",
       "4                                Women MPs endure \"shocking\" levels of sexist abuse   \n",
       "18     British troops on standby to help evacuate British citizens from Ivory Coast   \n",
       "17             David Beckham expresses relief at Real Madrid's passage to Champions   \n",
       "5       World and Commonwealth 100m champion Kim Collins will compete in the 60m at   \n",
       "19                                          Jurgen Melzer beat Andre Agassi 6-3 6-1   \n",
       "15                       Jamelia Davis reveals why she's still trying to make it in   \n",
       "22                                       20 years since the first mobile phone call   \n",
       "14                    Emails purporting to be from the FBI contain a computer virus   \n",
       "21       Thousands of public bodies are ill-prepared for the Freedom of Information   \n",
       "0   The explosion in consumer technology is to continue into 2005, delegates at the   \n",
       "20                                                     Sale vs Sale vs Sale vs Sale   \n",
       "6                                     Why people aren't sending multimedia messages   \n",
       "9                     Beijing officials to look at reorganising car parking charges   \n",
       "3                    Athletics Kenya reverses ban on marathon runner Susan Chepkeme   \n",
       "1          Cyberspace is becoming a very popular destination for Christmas shoppers   \n",
       "11                               The government shuns calls for free long-term care   \n",
       "7                       Pop Idol supremo Simon Fuller has sold his 19 Entertainment   \n",
       "2              Lord Falconer outlines plans for electronic tagging of uncooperative   \n",
       "16                          REM to play for 10,000 Scottish fans in rescheduled gig   \n",
       "12               The Kyrgyz Republic is using invisible ink and ultraviolet readers   \n",
       "10  Dollar regained some lost ground against most major currencies on Wednesday ...   \n",
       "24                       The UK is the fourth most digitally-savvy nation in Europe   \n",
       "13   Tony Blair says his personal standing will be an issue in the general election   \n",
       "\n",
       "    f_measure  \n",
       "8    0.500000  \n",
       "23   0.500000  \n",
       "4    0.461538  \n",
       "18   0.444444  \n",
       "17   0.352941  \n",
       "5    0.333333  \n",
       "19   0.333333  \n",
       "15   0.333333  \n",
       "22   0.266667  \n",
       "14   0.250000  \n",
       "21   0.235294  \n",
       "0    0.210526  \n",
       "20   0.181818  \n",
       "6    0.153846  \n",
       "9    0.142857  \n",
       "3    0.142857  \n",
       "1    0.133333  \n",
       "11   0.133333  \n",
       "7    0.133333  \n",
       "2    0.133333  \n",
       "16   0.125000  \n",
       "12   0.125000  \n",
       "10   0.111111  \n",
       "24   0.111111  \n",
       "13   0.095238  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('max_colwidth', 80) \n",
    "res.sort_values(\"f_measure\",ascending=False)\n",
    "\n",
    "# We can see how the first few generated titles do indeed seem to measure up to the actual titles quite well! Of course by no means \n",
    "# are the perfect replicas - indeed further training of the loaded model using the train dataset is likely to vastly improve performance\n",
    "# (though this is computationally expensive and is best done with a GPU) - yet it shows how ML models of today can be used in tandem with \n",
    "# our capabilities to create eye-catching titles than retain information very well. Indeed - this entire project aims to highlight the \n",
    "# benefits of AI when used by the right hands in improving our efficiency and abilities - which indeed can be extended to more than solely\n",
    "# News articles like above ;}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
