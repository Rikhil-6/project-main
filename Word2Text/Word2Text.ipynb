{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from PIL import Image\n",
    "import sounddevice as sd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dataset src: https://www.kaggle.com/datasets/warcoder/cats-vs-dogs-vs-birds-audio-classification?resource=download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word to Text\n",
    "\n",
    "Human speech is made up of numerous frequencies; which we interpret in a logarithmic way. Hence while frequencies that are a set distance apart from each other may not be distingusiahble to us, ones that are logarithmically distributed are identifiable. This is the basic premise behind the development of mel spectrograms vs normal ones. Spectrograms are also extremely helpful in sound classification as a form of encoding (and displaying) sound based information along time. This piece of code leverages the abilities of mel spectrograms to build a Convolution Neural Network (CNN) that can be used to classify the sounded audio of discrete words and thus output them as text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = os.path.join(os.curdir,r'Animals') # downloaded into the folder Animals (in the same directory as this file)\n",
    "assert os.path.exists(folder_path) # checks that the folder exists in directory\n",
    "r_state = 27 # to be used as random state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for k in os.listdir(folder_path): # for the folders (classes) and files in the Animals folder\n",
    "    subfolder = os.path.join(folder_path, k)\n",
    "    if os.path.isfile(subfolder): # skips files\n",
    "        pass \n",
    "    else:\n",
    "        for e in os.listdir(subfolder): \n",
    "            data.append((os.path.join(subfolder, e),k)) # and gets the complete (tho still relative) path of each file and the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dog     34.426230\n",
       "cat     33.934426\n",
       "bird    31.639344\n",
       "Name: Class, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_paths = pd.DataFrame(data) # creates a dataframe for those\n",
    "df_paths.columns = [\"Filepath\",\"Class\"]\n",
    "df_paths[\"Class\"].value_counts(normalize = True)*100 # pretty even split wrt classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_minmax(X, min=0.0, max=1.0): # normalization \n",
    "    X_std = (X - X.min()) / (X.max() - X.min())\n",
    "    X_scaled = X_std * (max - min) + min\n",
    "    return X_scaled\n",
    "\n",
    "def to_mel_img(file): # function that gets mel spectrogram of an audio file (in 2D array)\n",
    "    s,f = librosa.load(file)\n",
    "    melspectrogram = librosa.feature.melspectrogram(y=s,sr=f)\n",
    "    mels = np.log(melspectrogram+1e-10)\n",
    "    img_arr = scale_minmax(mels,0,255).astype(np.uint8) # normalized from 0 - 255 (img)\n",
    "    img_arr = np.flip(img_arr,axis=0) # flipped to have low values down below\n",
    "    return img_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 44)    536\n",
       "(128, 39)     15\n",
       "(128, 43)     14\n",
       "(128, 41)     13\n",
       "(128, 37)      8\n",
       "(128, 32)      6\n",
       "(128, 35)      5\n",
       "(128, 33)      5\n",
       "(128, 23)      2\n",
       "(128, 26)      2\n",
       "(128, 34)      2\n",
       "(128, 42)      1\n",
       "(128, 29)      1\n",
       "Name: Filepath, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = df_paths[\"Filepath\"].apply(lambda x: (to_mel_img(x).shape)).value_counts() # most of the arrays are of a specific shape\n",
    "counts # all arrays need to be standardised to this form (most common one chosen -> 128,44) : will redefine to_mel_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.51, 0.6, 0.65, 0.73, 0.74, 0.75, 0.77, 0.79, 0.81, 0.84, 0.85, 0.88, 0.89, 0.9, 0.93, 0.94, 0.97, 0.98, 1.0]\n"
     ]
    }
   ],
   "source": [
    "duration = df_paths[\"Filepath\"].apply(lambda x:librosa.get_duration(y=librosa.load(x)[0],sr=librosa.load(x)[1]))\n",
    "print(sorted(set(map(lambda x:round(x,2),duration.value_counts().index)))) # files are of length of about 1/2 to 1 second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.000000    536\n",
       "0.981406      9\n",
       "0.882404      7\n",
       "0.896009      7\n",
       "0.938730      7\n",
       "0.928844      6\n",
       "0.725397      6\n",
       "0.835964      5\n",
       "0.975283      5\n",
       "0.743084      4\n",
       "0.853379      3\n",
       "0.789524      3\n",
       "0.810703      2\n",
       "0.512018      2\n",
       "0.597415      2\n",
       "0.768027      2\n",
       "0.885397      1\n",
       "0.970703      1\n",
       "0.650204      1\n",
       "0.746712      1\n",
       "Name: Filepath, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duration.value_counts() # most of them hover at the 1 second mark --> ties in with the shape of such files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_mel_img(file,shape=None):\n",
    "    s,f = librosa.load(file)\n",
    "    melspectrogram = librosa.feature.melspectrogram(y=s,sr=f)\n",
    "    mels = np.log(melspectrogram+1e-10)\n",
    "    img_arr = scale_minmax(mels,0,255).astype(np.uint8)\n",
    "    if shape==None:\n",
    "        img_arr = np.flip(img_arr,axis=0)\n",
    "        return img_arr\n",
    "    else: # added code\n",
    "        img = Image.fromarray(np.uint8(img_arr)) # creates an image from the array\n",
    "        w,h = shape\n",
    "        resized_img = img.resize((h, w), Image.LANCZOS) # resizes the image accordingly \n",
    "        # (LANCZOS --> algo to smoothly interpolate values)\n",
    "        resized_arr = np.array(resized_img)\n",
    "        resized_arr = np.flip(resized_arr,axis=0)\n",
    "        return resized_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 44)    610\n",
       "Name: Filepath, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_paths[\"Filepath\"].apply(lambda x: (to_mel_img(x,(128,44))).shape).value_counts() # all values now standardised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_paths = pd.DataFrame({\"Img_arr\":df_paths[\"Filepath\"].apply(lambda x: (to_mel_img(x,(128,44))))}) # passed to a new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Img_arr</th>\n",
       "      <th>Encoded_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[91, 79, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[10, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[59, 44, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[40, 28, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[13, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>[[5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>[[39, 28, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>[[55, 42, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>[[67, 54, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>[[36, 24, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>610 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Img_arr  Encoded_class\n",
       "0    [[91, 79, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...              0\n",
       "1    [[10, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...              0\n",
       "2    [[59, 44, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...              0\n",
       "3    [[40, 28, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...              0\n",
       "4    [[13, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...              0\n",
       "..                                                 ...            ...\n",
       "605  [[5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...              2\n",
       "606  [[39, 28, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...              2\n",
       "607  [[55, 42, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...              2\n",
       "608  [[67, 54, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...              2\n",
       "609  [[36, 24, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...              2\n",
       "\n",
       "[610 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = sorted(set(df_paths[\"Class\"].values))\n",
    "encoding_dic = {k:i for i,k in enumerate(classes)} # with the class values encoded\n",
    "sf_paths[\"Encoded_class\"] = df_paths[\"Class\"].map(encoding_dic)\n",
    "\n",
    "sf_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bird': 0, 'cat': 1, 'dog': 2}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_dic # number to which each value maps to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, x_test, y, y_test = train_test_split(sf_paths[[\"Img_arr\"]],sf_paths[\"Encoded_class\"],test_size=0.2,train_size=0.8,\n",
    "    stratify=sf_paths[\"Encoded_class\"],random_state=r_state) # split into train and test\n",
    "x_train, x_val, y_train, y_val = train_test_split(x,y,test_size = 0.25,train_size =0.75,stratify=y,random_state=r_state)\n",
    "# from train split --> further split into a proper train set and validation set\n",
    "\n",
    "# x and y splits are wrapped together into single dfs shown below\n",
    "train_data = pd.concat([x_train,y_train],axis=1)\n",
    "val_data = pd.concat([x_val,y_val],axis=1)\n",
    "test_data = pd.concat([x_test,y_test],axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part on training the model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1dcd5285b70>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(r_state) # manual state set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module): # CNN model created by pytorch\n",
    "    def __init__(self,shape,n_classes):\n",
    "        super(CNN,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,16,3,padding=1)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.conv2 = nn.Conv2d(16,32,3,padding=1)\n",
    "        self.convs = nn.Sequential(\n",
    "            self.conv1,\n",
    "            nn.ReLU(),\n",
    "            self.pool,\n",
    "            self.conv2,\n",
    "            nn.ReLU(),\n",
    "            self.pool\n",
    "        )\n",
    "        self._to_linear = None\n",
    "        self._determine_linear_input(shape)\n",
    "        self.fc1 = nn.Linear(self._to_linear,128)\n",
    "        self.fc2 = nn.Linear(128,n_classes)\n",
    "    \n",
    "    def _determine_linear_input(self, shape):\n",
    "        # Automatic way to determine length of layers\n",
    "        with torch.no_grad():\n",
    "            o = self.convs(torch.zeros(1, *shape))\n",
    "        self._to_linear = int(o.numel())\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.pool(F.relu(self.conv1(x)))\n",
    "        x=self.pool(F.relu(self.conv2(x)))\n",
    "        x=x.view(-1,self._to_linear)\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=self.fc2(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset): #parse pd dataframe to be converted to tensors for pytorch\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.dataframe.iloc[idx]['Img_arr']\n",
    "        label = self.dataframe.iloc[idx]['Encoded_class']\n",
    "        image = image / 255.0 # normalisation of image\n",
    "        image = torch.tensor(image, dtype=torch.float32).unsqueeze(0) # number of channels (1) added to the front\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return image, label\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_torch = DataLoader(ImageDataset(train_data), batch_size=batch_size, shuffle=False)\n",
    "val_torch = DataLoader(ImageDataset(val_data), batch_size=batch_size, shuffle=False)\n",
    "test_torch = DataLoader(ImageDataset(test_data) , batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train, criterion, optimizer, device): # for training an epoch\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in tqdm(train, desc='Training'):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(train)\n",
    "\n",
    "def validate_one_epoch(model, val, criterion, device): # to run on the validation set per epoch\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0 # for accuracy testing\n",
    "    total = 0\n",
    "    all_preds = [] # for f1 score evaluation\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val, desc='Validation'):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())  # move to CPU and convert to numpy array\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "    f1 = f1_score(all_labels, all_preds,average='weighted')\n",
    "    return running_loss / len(val), accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 23/23 [00:01<00:00, 18.21it/s]\n",
      "Validation: 100%|██████████| 8/8 [00:00<00:00, 54.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Train Loss: 1.1195, Validation Loss: 1.0962, Validation Accuracy: 0.3443 Validation f1-score: 0.1763\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 23/23 [00:01<00:00, 18.31it/s]\n",
      "Validation: 100%|██████████| 8/8 [00:00<00:00, 38.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Train Loss: 1.0936, Validation Loss: 1.0851, Validation Accuracy: 0.4344 Validation f1-score: 0.3379\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 23/23 [00:01<00:00, 15.25it/s]\n",
      "Validation: 100%|██████████| 8/8 [00:00<00:00, 41.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Train Loss: 1.0620, Validation Loss: 1.0387, Validation Accuracy: 0.4754 Validation f1-score: 0.4334\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 23/23 [00:01<00:00, 16.56it/s]\n",
      "Validation: 100%|██████████| 8/8 [00:00<00:00, 29.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Train Loss: 0.9519, Validation Loss: 0.8990, Validation Accuracy: 0.5574 Validation f1-score: 0.5423\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 23/23 [00:01<00:00, 14.87it/s]\n",
      "Validation: 100%|██████████| 8/8 [00:00<00:00, 38.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Train Loss: 0.7734, Validation Loss: 0.7014, Validation Accuracy: 0.7459 Validation f1-score: 0.7411\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 23/23 [00:01<00:00, 17.40it/s]\n",
      "Validation: 100%|██████████| 8/8 [00:00<00:00, 39.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 - Train Loss: 0.6072, Validation Loss: 0.5436, Validation Accuracy: 0.8279 Validation f1-score: 0.8266\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 23/23 [00:01<00:00, 17.36it/s]\n",
      "Validation: 100%|██████████| 8/8 [00:00<00:00, 44.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 - Train Loss: 0.4711, Validation Loss: 0.4285, Validation Accuracy: 0.8852 Validation f1-score: 0.8846\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 23/23 [00:01<00:00, 12.58it/s]\n",
      "Validation: 100%|██████████| 8/8 [00:00<00:00, 35.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - Train Loss: 0.3929, Validation Loss: 0.3520, Validation Accuracy: 0.9344 Validation f1-score: 0.9342\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 23/23 [00:01<00:00, 15.45it/s]\n",
      "Validation: 100%|██████████| 8/8 [00:00<00:00, 33.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 - Train Loss: 0.3420, Validation Loss: 0.3219, Validation Accuracy: 0.9262 Validation f1-score: 0.9262\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 23/23 [00:01<00:00, 15.51it/s]\n",
      "Validation: 100%|██████████| 8/8 [00:00<00:00, 41.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 - Train Loss: 0.2862, Validation Loss: 0.2810, Validation Accuracy: 0.9180 Validation f1-score: 0.9185\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "shape = (1,*x_train.iloc[0].values[0].shape)\n",
    "classes = 3\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CNN(shape,classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch(model, train_torch, criterion, optimizer, device)\n",
    "    val_loss, val_accuracy, val_f1 = validate_one_epoch(model, val_torch, criterion, device)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs} - '\n",
    "          f'Train Loss: {train_loss:.4f}, '\n",
    "          f'Validation Loss: {val_loss:.4f}, '\n",
    "          f'Validation Accuracy: {val_accuracy:.4f}',\n",
    "          f'Validation f1-score: {val_f1:.4f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sound: Bird\n"
     ]
    }
   ],
   "source": [
    "def audio_pred(path,label_name_dic): # predict on an audio file given to it\n",
    "    mel_img = to_mel_img(path,(128,44))/255 # normalises the img_arr produced by the to_mel_img fn\n",
    "    image = torch.tensor(mel_img, dtype=torch.float32).unsqueeze(0).unsqueeze(0) # adds channel number and batch size sequentially\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "    predicted_classes = torch.argmax(output, dim=1)\n",
    "    return label_name_dic.get(predicted_classes.tolist()[0]) # model output is used as ref in dictionary to give the name of the class\n",
    "\n",
    "file = os.path.join(folder_path, \"test_rec1.mp3\") # gets the filpath of one of the test recordings (can change as wished)\n",
    "\n",
    "prediction = audio_pred(file, {v:k for k,v in encoding_dic.items()})\n",
    "y,sr = librosa.load(file) # playes the audio of the file \n",
    "sd.play(y,sr)\n",
    "sd.wait()\n",
    "print(f\"Predicted Sound: {prediction.title()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "The test recordings 1 through 3 in the Animals folder help to determine the performance of the model from an external perspective --> and indicate how useful the visualisation of audio through mel spectrograms into model training (as a CNN) can be. Recordings 4 through 6, while helping to further prove the skill of the model by wrapping similar sounding words in with the classes given to the model to be trained; also highlight the limitations of the approach taken. While no \"other\" category has been shown, it is clear that the model is not sensitive to every phoneme of the spoken word. \n",
    "\n",
    "This can be beneficial to accomodate a wide range of pronunciations; but is also (in the case of the test recordings) proof that there ought to be a better approach of creating a model that can determine how the word sounds intrinsically and from there try and guess the correct word. I aim to investigate this by checking if a phoneme database is available/ could be made to split similar sounding words or even look into other approaches that can distinguish speech more effectively. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
